DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
Thu Apr  4 12:54:54 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100S-PCIE-32GB          On  | 00000000:06:00.0 Off |                    0 |
| N/A   29C    P0              26W / 250W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

RUN: 1
  1.1. Training the classifier...
Map:   0%|          | 0/1503 [00:00<?, ? examples/s]Map:  67%|██████▋   | 1000/1503 [00:00<00:00, 1890.39 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1924.06 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1908.38 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2039.56 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2012.00 examples/s]
Training Loss : 0.6284024064686704 | Validation Loss : 0.5051103234291077
Training Loss : 0.3512345354488872 | Validation Loss : 0.46901214122772217
Training Loss : 0.33100987413026234 | Validation Loss : 0.4863876402378082
Training Loss : 0.3238829566287174 | Validation Loss : 0.4816488027572632
Training Loss : 0.3123800292341138 | Validation Loss : 0.46639442443847656
Training Loss : 0.30822851929932515 | Validation Loss : 0.44409215450286865
Training Loss : 0.29820527672331704 | Validation Loss : 0.4881978929042816
Training Loss : 0.2955228962881015 | Validation Loss : 0.4875795841217041
Training Loss : 0.3008559167043286 | Validation Loss : 0.45579153299331665
Training Loss : 0.28897056298855534 | Validation Loss : 0.46594008803367615
Training Loss : 0.2818050647805881 | Validation Loss : 0.4481719732284546
Training Loss : 0.2879389003662274 | Validation Loss : 0.46578845381736755
Training Loss : 0.2882078608567331 | Validation Loss : 0.44464433193206787
Training Loss : 0.2799421315913663 | Validation Loss : 0.4580006003379822
Training Loss : 0.27616922914278397 | Validation Loss : 0.4484191834926605
Training Loss : 0.26910496891743724 | Validation Loss : 0.477795273065567
Training Loss : 0.27230256247492707 | Validation Loss : 0.4515891671180725
Training Loss : 0.26665171081239875 | Validation Loss : 0.45931077003479004
Training Loss : 0.26981027794387585 | Validation Loss : 0.45882511138916016
Training Loss : 0.25999383113168656 | Validation Loss : 0.4569178521633148
Training Loss : 0.2607274528647317 | Validation Loss : 0.4556800127029419

  1.2. Eval on the dev set...Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1944.70 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1919.14 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Test Loss : 0.44409215450286865
 Acc.: 85.90


RUN: 2
  2.1. Training the classifier...
Map:   0%|          | 0/1503 [00:00<?, ? examples/s]Map:  67%|██████▋   | 1000/1503 [00:00<00:00, 1983.95 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1973.40 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1967.38 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2042.98 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2013.68 examples/s]
Training Loss : 0.5749682209830969 | Validation Loss : 0.4514389634132385
Training Loss : 0.3423277923398889 | Validation Loss : 0.4104200601577759
Training Loss : 0.30830408628544514 | Validation Loss : 0.3858191967010498
Training Loss : 0.30420495919447316 | Validation Loss : 0.4007161259651184
Training Loss : 0.28904260692008316 | Validation Loss : 0.3953346908092499
Training Loss : 0.27560478927904464 | Validation Loss : 0.42237040400505066
Training Loss : 0.2702342725007181 | Validation Loss : 0.3897147476673126
Training Loss : 0.2603437195413132 | Validation Loss : 0.40131720900535583
Training Loss : 0.25773128761880176 | Validation Loss : 0.3910881578922272
Training Loss : 0.25542242128025505 | Validation Loss : 0.3995310366153717
Training Loss : 0.25393815862604396 | Validation Loss : 0.42782050371170044
Training Loss : 0.25597770296116457 | Validation Loss : 0.41243788599967957
Training Loss : 0.24022993222473465 | Validation Loss : 0.4105398952960968
Training Loss : 0.24279155005066794 | Validation Loss : 0.40766897797584534
Training Loss : 0.23891493715866013 | Validation Loss : 0.42893245816230774
Training Loss : 0.23569891354356437 | Validation Loss : 0.4261413514614105
Training Loss : 0.2231976087205112 | Validation Loss : 0.42129409313201904
Training Loss : 0.21724984073327816 | Validation Loss : 0.42219278216362
Training Loss : 0.2261716056351322 | Validation Loss : 0.41738659143447876
Training Loss : 0.22538600346454954 | Validation Loss : 0.4207037091255188
Training Loss : 0.2210724381457499 | Validation Loss : 0.42016592621803284

  2.2. Eval on the dev set...Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1965.22 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1939.46 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Test Loss : 0.3858191967010498
 Acc.: 86.44


RUN: 3
  3.1. Training the classifier...
Map:   0%|          | 0/1503 [00:00<?, ? examples/s]Map:  67%|██████▋   | 1000/1503 [00:00<00:00, 1976.81 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1970.51 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1963.87 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2039.59 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2011.39 examples/s]
Training Loss : 0.5796273587707509 | Validation Loss : 0.46088919043540955
Training Loss : 0.3472380212314942 | Validation Loss : 0.4465821385383606
Training Loss : 0.33179480928253935 | Validation Loss : 0.497202068567276
Training Loss : 0.32506842647451273 | Validation Loss : 0.48183321952819824
Training Loss : 0.30973537979360194 | Validation Loss : 0.47235527634620667
Training Loss : 0.30274708432323755 | Validation Loss : 0.48535552620887756
Training Loss : 0.2949033748517011 | Validation Loss : 0.4765493869781494
Training Loss : 0.27968355895891944 | Validation Loss : 0.4663054943084717
Training Loss : 0.2791309400085122 | Validation Loss : 0.4840656816959381
Training Loss : 0.27246909652975326 | Validation Loss : 0.4618806838989258
Training Loss : 0.2765916775357216 | Validation Loss : 0.4547429382801056
Training Loss : 0.2691423732420786 | Validation Loss : 0.460288405418396
Training Loss : 0.2620752618989927 | Validation Loss : 0.4797188937664032
Training Loss : 0.2584582073589232 | Validation Loss : 0.4596771001815796
Training Loss : 0.2565647707657611 | Validation Loss : 0.47994616627693176
Training Loss : 0.252764583525664 | Validation Loss : 0.4599737823009491
Training Loss : 0.2476510164981827 | Validation Loss : 0.456061989068985
Training Loss : 0.24620043500722882 | Validation Loss : 0.4604012668132782
Training Loss : 0.2403739915994749 | Validation Loss : 0.46367040276527405
Training Loss : 0.2394583115274919 | Validation Loss : 0.4632529616355896
Training Loss : 0.2401601746876506 | Validation Loss : 0.46654510498046875

  3.2. Eval on the dev set...Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1963.06 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1937.12 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Test Loss : 0.4465821385383606
 Acc.: 84.04


RUN: 4
  4.1. Training the classifier...
Map:   0%|          | 0/1503 [00:00<?, ? examples/s]Map:  67%|██████▋   | 1000/1503 [00:00<00:00, 1983.52 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1967.94 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1962.81 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2036.68 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2008.88 examples/s]
Training Loss : 0.5672001566817152 | Validation Loss : 0.4207639694213867
Training Loss : 0.31137253623455763 | Validation Loss : 0.43663278222084045
Training Loss : 0.2827322416384011 | Validation Loss : 0.41451436281204224
Training Loss : 0.2783581944915684 | Validation Loss : 0.40907949209213257
Training Loss : 0.26022448675568277 | Validation Loss : 0.4226405918598175
Training Loss : 0.2540816933355056 | Validation Loss : 0.3986976146697998
Training Loss : 0.2595167695742814 | Validation Loss : 0.40490883588790894
Training Loss : 0.252487027039256 | Validation Loss : 0.4542892277240753
Training Loss : 0.2520262197705303 | Validation Loss : 0.41155314445495605
Training Loss : 0.24766444938475304 | Validation Loss : 0.4183937609195709
Training Loss : 0.23226997584161507 | Validation Loss : 0.4206997752189636
Training Loss : 0.23607711969909476 | Validation Loss : 0.4156639575958252
Training Loss : 0.23535208572494856 | Validation Loss : 0.42087456583976746
Training Loss : 0.22223961392685057 | Validation Loss : 0.43433353304862976
Training Loss : 0.22189536563150505 | Validation Loss : 0.42216822504997253
Training Loss : 0.21679567497104724 | Validation Loss : 0.41309598088264465
Training Loss : 0.21570057707016377 | Validation Loss : 0.42171111702919006
Training Loss : 0.2064721558188544 | Validation Loss : 0.4254755675792694
Training Loss : 0.211748647055232 | Validation Loss : 0.4213113486766815
Training Loss : 0.20786015447873146 | Validation Loss : 0.42223331332206726
Training Loss : 0.20626381542929945 | Validation Loss : 0.42248132824897766

  4.2. Eval on the dev set...Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1980.55 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1954.24 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Test Loss : 0.3986976146697998
 Acc.: 85.90


RUN: 5
  5.1. Training the classifier...
Map:   0%|          | 0/1503 [00:00<?, ? examples/s]Map:  67%|██████▋   | 1000/1503 [00:00<00:00, 1986.10 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1975.07 examples/s]Map: 100%|██████████| 1503/1503 [00:00<00:00, 1969.19 examples/s]
Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2037.72 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 2010.24 examples/s]
Training Loss : 0.6441213077211634 | Validation Loss : 0.5036206841468811
Training Loss : 0.4366620592921576 | Validation Loss : 0.477758526802063
Training Loss : 0.3973956456645689 | Validation Loss : 0.5089877247810364
Training Loss : 0.38607173208939904 | Validation Loss : 0.4715544879436493
Training Loss : 0.3824728052528139 | Validation Loss : 0.4523908495903015
Training Loss : 0.36449268909091964 | Validation Loss : 0.5190314054489136
Training Loss : 0.37150216921470425 | Validation Loss : 0.45446816086769104
Training Loss : 0.359518466586683 | Validation Loss : 0.4500490128993988
Training Loss : 0.354668175130389 | Validation Loss : 0.4498246908187866
Training Loss : 0.3548551327548921 | Validation Loss : 0.4550069272518158
Training Loss : 0.3518145681913704 | Validation Loss : 0.46150314807891846
Training Loss : 0.35639182081386606 | Validation Loss : 0.45331764221191406
Training Loss : 0.3414832171191402 | Validation Loss : 0.45161768794059753
Training Loss : 0.3449800631249363 | Validation Loss : 0.45949310064315796
Training Loss : 0.3399916664508984 | Validation Loss : 0.45513737201690674
Training Loss : 0.3318361122745703 | Validation Loss : 0.44962090253829956
Training Loss : 0.329598552111457 | Validation Loss : 0.45308420062065125
Training Loss : 0.33348024602820897 | Validation Loss : 0.45544350147247314
Training Loss : 0.3324171612844208 | Validation Loss : 0.4524068534374237
Training Loss : 0.322712689568784 | Validation Loss : 0.45238742232322693
Training Loss : 0.32869685672104676 | Validation Loss : 0.45212313532829285

  5.2. Eval on the dev set...Map:   0%|          | 0/376 [00:00<?, ? examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1963.96 examples/s]Map: 100%|██████████| 376/376 [00:00<00:00, 1937.53 examples/s]
Test Loss : 0.44962090253829956
 Acc.: 84.31


Completed 5 runs.
Dev accs: [85.9, 86.44, 84.04, 85.9, 84.31]
Test accs: [-1, -1, -1, -1, -1]

Mean Dev Acc.: 85.32 (0.96)
Mean Test Acc.: -1.00 (0.00)

Exec time: 1167.21 s. ( 233 per run )
